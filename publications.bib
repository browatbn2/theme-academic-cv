@inproceedings{Browatzki2020a,
abstract = {Current supervised methods for facial landmark detection require a large amount of training data and may suffer from overfitting to specific datasets due to the massive number of parameters. We introduce a semi-supervised method in which the crucial idea is to first generate implicit face knowledge from the large amounts of unlabeled images of faces available today. In a first, completely unsupervised stage, we train an adversarial autoencoder to reconstruct faces via a low-dimensional face embedding. In a second, supervised stage, we interleave the decoder with transfer layers to retask the generation of color images to the prediction of landmark heatmaps. Our framework (3FabRec) achieves state-of-the-art performance on several common benchmarks and, most importantly, is able to maintain impressive accuracy on extremely small training sets down to as few as 10 images. As the interleaved layers only add a low amount of parameters to the decoder, inference runs at several hundred FPS on a GPU.},
archivePrefix = {arXiv},
arxivId = {1911.10448},
author = {Browatzki, Bjorn and Wallraven, Christian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR42600.2020.00615},
eprint = {1911.10448},
file = {:home/browatbn/Dropbox/Papers/MyPublications/1911.10448-1.pdf:pdf},
issn = {10636919},
pages = {6109--6119},
title = {{3FabRec: Fast Few-Shot Face Alignment by Reconstruction}},
year = {2020}
}
@article{Browatzki2020,
abstract = {We propose an encoder-decoder framework for the segmentation of blood vessels in retinal images that relies on the extraction of large-scale patches at multiple image-scales during training. Experiments on three fundus image datasets demonstrate that this approach achieves state-of-the-art results and can be implemented using a simple and efficient fully-convolutional network with a parameter count of less than 0.8M. Furthermore, we show that this framework - called VLight - avoids overfitting to specific training images and generalizes well across different datasets, which makes it highly suitable for real-world applications where robustness, accuracy as well as low inference time on high-resolution fundus images is required.},
author = {Browatzki, Bj{\"{o}}rn and Lies, J{\"{o}}rn Philipp and Wallraven, Christian},
doi = {10.1007/978-3-030-63419-3_5},
file = {:home/browatbn/Dropbox/Papers/browatzki_OMIA_2020.pdf:pdf},
isbn = {9783030634186},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Fundus image,Residual networks,Retinal vessel detection,Semantic segmentation},
pages = {42--52},
title = {{Encoder-Decoder Networks for Retinal Vessel Segmentation Using Large Multi-scale Patches}},
volume = {12069 LNCS},
year = {2020}
}
@article{Browatzki2019,
abstract = {Current solutions to discriminative and generative tasks in computer vision exist separately and often lack interpretability and explainability. Using faces as our application domain, here we present an architecture that is based around two core ideas that address these issues: first, our framework learns an unsupervised, low-dimensional embedding of faces using an adversarial autoencoder that is able to synthesize high-quality face images. Second, a supervised disentanglement splits the low-dimensional embedding vector into four sub-vectors, each of which contains separated information about one of four major face attributes (pose, identity, expression, and style) that can be used both for discriminative tasks and for manipulating all four attributes in an explicit manner. The resulting architecture achieves state-of-the-art image quality, good discrimination and face retrieval results on each of the four attributes, and supports various face editing tasks using a face representation of only 99 dimensions. Finally, we apply the architecture's robust image synthesis capabilities to visually debug label-quality issues in an existing face dataset.},
author = {Browatzki, Bjoern and Wallraven, Christian},
doi = {10.1109/ICCVW.2019.00071},
file = {:home/browatbn/Dropbox/Papers/MyPublications/rslcv2009_final_browatzki.pdf:pdf},
isbn = {9781728150239},
journal = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
keywords = {Adversarial autoencoder,Disentanglement,Face identification,Facial expression recognition,Image generation},
pages = {579--588},
title = {{Robust discrimination and generation of faces using compact, disentangled embeddings}},
year = {2019}
}
@article{moon2019deep,
author = {Moon, Hyungjun and Browatzki, Bj{\"{o}}rn and Blais, Caroline and Wallraven, Christian},
journal = {IBRO Reports},
pages = {S193----S194},
publisher = {Elsevier},
title = {{Deep neural networks process similar facial features compared to humans in facial expression recognition}},
volume = {6},
year = {2019}
}
@article{Ryu2018,
abstract = {Purpose : The aim of this study is to develop a novel deep learning system for vessel segmentation of retinal images. We present a recurrent Convolutional Neural Network (CNN) architecture and compare performance with existing CNN approaches, showing greatly reduced processing time with excellent performance. Methods : The proposed DirectNet architecture is composed of blocks, with each block containing a collection of convolutional layers. Blocks are stacked up in a pyramid, such that the number of blocks is increased by one at each level. Data are repeatedly processed by each block and combined with outputs of other blocks. This recurrent structure combined with the use of large kernel avoids the need for up-or downsampling layers, thus creating a direct pixel-to-pixel mapping from pixel inputs to the outputs of segmentation. Results : DirectNet provides higher accuracy, sensitivity, specificity, and precision values compared to a state-of-the-art, patch-based CNN approach (0.9538 vs 0.9327, 0.7851 vs 0.7346, 0.9782 vs 0.9730, 0.8458 vs 0.7987). Training time on a standard dataset for DirectNet is reduced from 8 hours to 1 hour, and testing time per image is greatly reduced from 1 hour for the patch-based method to 6 seconds for our method. Conclusion : The proposed deep-learning architecture is eight times faster for training and 600 times faster for testing at slightly higher accuracy values than a state-of-the-art method. Segmentation successfully highlights retinal blood vessels of large down to small sizes.},
author = {Ryu, Hyeongsuk and Moon, Hyeongjun and Browatzki, Bj{\"{o}}rn and Wallraven, Christian},
doi = {10.17337/jmbi.2018.20.2.151},
file = {:home/browatbn/Dropbox/Papers/MyPublications/00020_002_151.pdf:pdf},
issn = {1229-6457},
journal = {The Korean Journal of Vision Science},
keywords = {82-2-3290-5925,address reprint request to,anam-dong,christian,christian wallraven,com,deep learning,e-mail,gmail,korea,korea university,machine learning,retinal vessel detection,seoul 136-701,sungbuk-ku,tel,wallraven},
number = {2},
pages = {151--159},
title = {{Retinal Vessel Detection Using Deep Learning: A novel DirectNet Architecture}},
volume = {20},
year = {2018}
}
@article{Browatzki2014a,
abstract = {For any robot, the ability to recognize and manipulate unknown objects is crucial to successfully work in natural environments. Object recognition and categorization is a very challenging problem, as 3-D objects often give rise to ambiguous, 2-D views. Here, we present a perception-driven exploration and recognition scheme for in-hand object recognition implemented on the iCub humanoid robot. In this setup, the robot actively seeks out object views to optimize the exploration sequence. This is achieved by regarding the object recognition problem as a localization problem. We search for the most likely viewpoint position on the viewsphere of all objects. This problem can be solved efficiently using a particle filter that fuses visual cues with associated motor actions. Based on the state of the filter, we can predict the next best viewpoint after each recognition step by searching for the action that leads to the highest expected information gain. We conduct extensive evaluations of the proposed system in simulation as well as on the actual robot and show the benefit of perception-driven exploration over passive, vision-only processes at discriminating between highly similar objects. We demonstrate that objects are recognized faster and at the same time with a higher accuracy.},
author = {Browatzki, Bjorn and Tikhanoff, Vadim and Metta, Giorgio and Bulthoff, Heinrich H. and Wallraven, Christian},
doi = {10.1109/TRO.2014.2328779},
file = {:home/browatbn/Dropbox/Papers/MyPublications/t-ro_rev.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
number = {5},
pages = {1260--1269},
title = {{Active in-hand object recognition on a humanoid robot}},
volume = {30},
year = {2014}
}
@phdthesis{browatzki2014multimodal,
author = {Browatzki, Bj{\"{o}}rn},
school = {Universit{\"{a}}t Stuttgart, Fakult{\"{a}}t Konstruktions-, Produktions-und$\sim${\ldots}},
title = {{Multimodal object perception for robotics}},
year = {2014}
}
@article{Browatzki2014,
abstract = {Video-based gaze-tracking systems are typically restricted in terms of their effective tracking space. This constraint limits the use of eyetrackers in studying mobile human behavior. Here, we compare two possible approaches for estimating the gaze of participants who are free to walk in a large space whilst looking at different regions of a large display. Geometrically, we linearly combined eye-in-head rotations and head-in-world coordinates to derive a gaze vector and its intersection with a planar display, by relying on the use of a head-mounted eyetrackerand body-motion tracker. Alternatively, we employed Gaussian process regression to estimate the gaze intersection directly from the input data itself. Our evaluation of both methods indicates that a regression approach can deliver comparable results to a geometric approach. The regression approach is favored, given that it has the potential for further optimization, provides confidence bounds for its gaze estimates and offers greater flexibility in its implementation. Open-source software for the methods reported here is also provided for user implementation. {\textcopyright} 2014 Browatzki, Bulthoff and Chuang.},
author = {Browatzki, Bj{\"{o}}rn and B{\"{u}}lthoff, Heinrich H. and Chuang, Lewis L.},
doi = {10.3389/fnhum.2014.00200},
file = {:home/browatbn/Dropbox/Papers/MyPublications/fnhum-08-00200.pdf:pdf},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {Active vision,Calibration method,Eye movement,Eye tracking,Gaussian processes,Gaze measurement,calibration method, gaze measurement, eye tracking},
number = {1 APR},
pages = {1--12},
title = {{A comparison of geometric- and regression-based mobile gaze-tracking}},
volume = {8},
year = {2014}
}
@inproceedings{Browatzki2012,
author = {Browatzki, Bj{\"{o}}rn and Tikhanoff, Vadim and Metta, Giorgio and B{\"{u}}lthoff, Heinrich H. and Wallraven, Christian},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2012.6225218},
file = {:home/browatbn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browatzki et al. - 2012 - Active object recognition on a humanoid robot.pdf:pdf},
isbn = {978-1-4673-1405-3},
month = {may},
pages = {2021 -- 2028},
publisher = {IEEE},
title = {{Active object recognition on a humanoid robot}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6225218},
year = {2012}
}
@inproceedings{Browatzki2011,
abstract = {Categorization of objects solely based on shape and appearance is still a largely unresolved issue. With the advent of new sensor technologies*such as consumer-level range sensors*new possibilities for shape processing have become available for a range of new application domains. In the first part of this paper*we introduce a novel*large dataset containing 18 categories of objects found in typical household and office environmentswe envision this dataset to be useful in many applications ranging from robotics to computer vision. The second part of the paper presents computational experiments on object categorization with classifiers exploiting both two-dimensional and three-dimensional information. We evaluate categorization performance for both modalities in separate and combined representations and demonstrate the advantages of using range data for object and shape processing skills. {\textcopyright} 2011 IEEE.},
author = {Browatzki, Bj{\"{o}}rn and Fischer, Jan and Graf, Birgit and Bulthoff, Heinrich H. and Wallraven, Christian},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCVW.2011.6130385},
file = {:home/browatbn/Dropbox/Papers/MyPublications/ICRA2011.pdf:pdf},
isbn = {9781467300629},
pages = {1189--1196},
title = {{Going into depth: Evaluating 2D and 3D cues for object classification on a new large-scale object dataset}},
year = {2011}
}
@inproceedings{Engel2011,
abstract = {With increasingly large image databases, searching in them becomes an ever more difficult endeavor. Consequently, there is a need for advanced tools for image retrieval in a webscale context. Searching by tags becomes intractable in such scenarios as large numbers of images will correspond to queries such as "car and house and street". We present a novel approach that allows a user to search for images based on semantic sketches that describe the desired composition of the image. Our system operates on images with labels for a few high-level object categories, allowing us to search very fast with a minimal memory footprint. We employ a structure similar to random decision forests which avails a data-driven partitioning of the image space providing a search in logarithmic time with respect to the number of images. This makes our system applicable for large scale image search problems. We performed a user study that demonstrates the validity and usability of our approach. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
author = {Engel, David and Herdtweck, Christian and Browatzki, Bj{\"{o}}rn and Curio, Crist{\'{o}}bal},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-23774-4_35},
file = {:home/browatbn/Dropbox/Papers/MyPublications/EngelHBC11.pdf:pdf},
isbn = {9783642237737},
issn = {03029743},
keywords = {Content-Based Image Retrieval,Real-Time Application,Semantic Brushes,Sketch Interface,User-Study},
number = {PART 1},
pages = {412--425},
title = {{Image retrieval with semantic sketches}},
volume = {6946 LNCS},
year = {2011}
}
